{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "data = pd.read_csv(\"data/merged_data.csv\")\n",
    "\n",
    "# Realizar el preprocesamiento necesario en las columnas de texto (por ejemplo, eliminación de signos de puntuación, tokenización)\n",
    "# Puedes utilizar bibliotecas como nltk o spaCy para el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\python39\\lib\\site-packages (from nltk) (8.1.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python39\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "0     The third wave was an experimentto see how peo...   \n",
      "1     The Third Wave developed  rapidly because the ...   \n",
      "2     The third wave only started as an experiment w...   \n",
      "3     The experimen was orginally about how even whe...   \n",
      "4     The third wave developed so quickly due to the...   \n",
      "...                                                 ...   \n",
      "7160  It has to be made on a complex storyline, with...   \n",
      "7161  Aristotle descirbes an ideal tradgedy as being...   \n",
      "7162  A tragedy should have a complex plan not a sim...   \n",
      "7163  Aristotle believed that the ideal tradegy shou...   \n",
      "7164  An ideal tragety has three elements that make ...   \n",
      "\n",
      "                                      preprocessed_text  \n",
      "0     third wave experimentto see people reacted new...  \n",
      "1     third wave developed rapidly students genuinly...  \n",
      "2     third wave started experiment within class slo...  \n",
      "3     experimen orginally even terrible thngs happen...  \n",
      "4     third wave developed quickly due students part...  \n",
      "...                                                 ...  \n",
      "7160  made complex storyline plot makes audience fee...  \n",
      "7161  aristotle descirbes ideal tradgedy one complex...  \n",
      "7162  tragedy complex plan simple onea good plot plo...  \n",
      "7163  aristotle believed ideal tradegy include purga...  \n",
      "7164  ideal tragety three elements make ideal start ...  \n",
      "\n",
      "[7165 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Descargar recursos adicionales de NLTK (si aún no se han descargado)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Crear una lista de palabras de detención y definir un tokenizador\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    # Convierte el texto a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Elimina signos de puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenización\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Elimina palabras de detención\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Reconstruye el texto preprocesado\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Aplica la función de preprocesamiento a la columna 'text' del DataFrame\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Muestra el DataFrame resultante con el texto preprocesado\n",
    "print(data[['text', 'preprocessed_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Representaciones de texto con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT pre-entrenado\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizar tus textos y ajustar la longitud de las secuencias\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in data['text']:\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids.append(encoding['input_ids'])\n",
    "    attention_masks.append(encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Construcción del modelo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir un modelo de regresión basado en BERT\n",
    "class BERTRegressionModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BERTRegressionModel, self).__init__()\n",
    "        self.bert = model\n",
    "        self.regression_head = nn.Linear(768, 1)  # Salida de BERT (768 dimensiones) a una sola neurona para regresión\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        regression_output = self.regression_head(pooled_output)\n",
    "        return regression_output\n",
    "\n",
    "# Crear una instancia del modelo de regresión\n",
    "regression_model = BERTRegressionModel(model)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador para regresión\n",
    "criterion = nn.MSELoss()  # Error cuadrático medio\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 71] at entry 0 and [1, 263] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jose\\Documents\\GitHub\\Fork\\Proyecto_DataScience\\proyecto.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Crear conjuntos de datos de PyTorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(input_ids)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mask_train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(attention_masks)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_content \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)  \u001b[39m# Reemplaza 'content' con tu columna de contenido\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 71] at entry 0 and [1, 263] at entry 1"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Crear conjuntos de datos de PyTorch\n",
    "X_train_tensor = torch.stack(input_ids)\n",
    "mask_train_tensor = torch.stack(attention_masks)\n",
    "y_content = torch.tensor(data['content'], dtype=torch.float32)  # Reemplaza 'content' con tu columna de contenido\n",
    "y_wording = torch.tensor(data['wording'], dtype=torch.float32)  # Reemplaza 'wording' con tu columna de redacción\n",
    "\n",
    "dataset_content = TensorDataset(X_train_tensor, mask_train_tensor, y_content)\n",
    "dataset_wording = TensorDataset(X_train_tensor, mask_train_tensor, y_wording)\n",
    "\n",
    "# Define un DataLoader para cargar datos de entrenamiento en lotes\n",
    "batch_size = 32\n",
    "dataloader_content = DataLoader(dataset_content, batch_size=batch_size)\n",
    "dataloader_wording = DataLoader(dataset_wording, batch_size=batch_size)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_regression_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, masks, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, masks)\n",
    "            loss = criterion(outputs.view(-1), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Entrenar el modelo para 'content'\n",
    "train_regression_model(regression_model, dataloader_content, criterion, optimizer)\n",
    "\n",
    "# Entrenar el modelo para 'wording'\n",
    "train_regression_model(regression_model, dataloader_wording, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test, attention_masks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluación, generación de comentarios y presentación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo, comparar las predicciones con los valores reales y generar comentarios.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Generar comentarios\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= y_test[i]:\n",
    "        print(f\"Para el ejemplo {i+1}, el contenido es bueno.\")\n",
    "    else:\n",
    "        print(f\"Para el ejemplo {i+1}, el contenido se puede mejorar.\")\n",
    "\n",
    "# Muestra los comentarios junto con las predicciones para proporcionar retroalimentación completa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
