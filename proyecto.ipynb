{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "data = pd.read_csv(\"data/merged_data.csv\")\n",
    "\n",
    "# Realizar el preprocesamiento necesario en las columnas de texto (por ejemplo, eliminación de signos de puntuación, tokenización)\n",
    "# Puedes utilizar bibliotecas como nltk o spaCy para el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\python39\\lib\\site-packages (from nltk) (8.1.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python39\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "0     The third wave was an experimentto see how peo...   \n",
      "1     The Third Wave developed  rapidly because the ...   \n",
      "2     The third wave only started as an experiment w...   \n",
      "3     The experimen was orginally about how even whe...   \n",
      "4     The third wave developed so quickly due to the...   \n",
      "...                                                 ...   \n",
      "7160  It has to be made on a complex storyline, with...   \n",
      "7161  Aristotle descirbes an ideal tradgedy as being...   \n",
      "7162  A tragedy should have a complex plan not a sim...   \n",
      "7163  Aristotle believed that the ideal tradegy shou...   \n",
      "7164  An ideal tragety has three elements that make ...   \n",
      "\n",
      "                                      preprocessed_text  \n",
      "0     third wave experimentto see people reacted new...  \n",
      "1     third wave developed rapidly students genuinly...  \n",
      "2     third wave started experiment within class slo...  \n",
      "3     experimen orginally even terrible thngs happen...  \n",
      "4     third wave developed quickly due students part...  \n",
      "...                                                 ...  \n",
      "7160  made complex storyline plot makes audience fee...  \n",
      "7161  aristotle descirbes ideal tradgedy one complex...  \n",
      "7162  tragedy complex plan simple onea good plot plo...  \n",
      "7163  aristotle believed ideal tradegy include purga...  \n",
      "7164  ideal tragety three elements make ideal start ...  \n",
      "\n",
      "[7165 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Descargar recursos adicionales de NLTK (si aún no se han descargado)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Crear una lista de palabras de detención y definir un tokenizador\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    # Convierte el texto a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Elimina signos de puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenización\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Elimina palabras de detención\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Reconstruye el texto preprocesado\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Aplica la función de preprocesamiento a la columna 'text' del DataFrame\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Muestra el DataFrame resultante con el texto preprocesado\n",
    "print(data[['text', 'preprocessed_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Representaciones de texto con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT pre-entrenado\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizar tus textos y ajustar la longitud de las secuencias\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in data['text']:\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids.append(encoding['input_ids'])\n",
    "    attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jose\\Documents\\GitHub\\Fork\\Proyecto_DataScience\\proyecto.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(padded_input_ids), torch\u001b[39m.\u001b[39mstack(attention_masks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Aplicar el relleno y crear las máscaras de atención\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m input_ids, attention_masks \u001b[39m=\u001b[39m pad_and_create_mask(input_ids)\n",
      "\u001b[1;32mc:\\Users\\Jose\\Documents\\GitHub\\Fork\\Proyecto_DataScience\\proyecto.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m max_sequence_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     padded_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((ids, torch\u001b[39m.\u001b[39;49mzeros(max_sequence_length \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(ids))))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((torch\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(ids))), torch\u001b[39m.\u001b[39mzeros(max_sequence_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(ids)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jose/Documents/GitHub/Fork/Proyecto_DataScience/proyecto.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m padded_input_ids\u001b[39m.\u001b[39mappend(padded_ids)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define la longitud máxima deseada para tus secuencias (ajusta según tus necesidades)\n",
    "max_sequence_length = 256\n",
    "\n",
    "# Función para aplicar el relleno\n",
    "def pad_and_create_mask(input_ids):\n",
    "    padded_input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for ids in input_ids:\n",
    "        if len(ids) > max_sequence_length:\n",
    "            padded_ids = ids[:max_sequence_length]\n",
    "            mask = torch.tensor([1] * max_sequence_length)\n",
    "        else:\n",
    "            padded_ids = torch.cat((ids, torch.zeros(max_sequence_length - len(ids), dtype=torch.long)))\n",
    "            mask = torch.cat((torch.ones(len(ids), dtype=torch.long)), torch.zeros(max_sequence_length - len(ids), dtype=torch.long))\n",
    "        \n",
    "        padded_input_ids.append(padded_ids)\n",
    "        attention_masks.append(mask)\n",
    "\n",
    "    return torch.stack(padded_input_ids), torch.stack(attention_masks)\n",
    "\n",
    "# Aplicar el relleno y crear las máscaras de atención\n",
    "input_ids, attention_masks = pad_and_create_mask(input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Construcción del modelo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define un modelo de regresión que utiliza BERT como base\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.bert = model.bert\n",
    "        self.linear = nn.Linear(768, 1)  # 768 es la dimensión de salida de BERT\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        output = self.bert(input_ids, attention_mask=attention_masks)[0]\n",
    "        output = output[:, 0, :]  # Tomar la representación [CLS] de BERT\n",
    "        output = self.linear(output)\n",
    "        output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "model = Regressor()\n",
    "criterion = nn.MSELoss()  # Error cuadrático medio como función de pérdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_ids, data['content'], test_size=0.2, random_state=42)\n",
    "attention_masks_train, attention_masks_test, _, _ = train_test_split(attention_masks, data['content'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "epochs = 5  # Número de épocas (ajusta según sea necesario)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train, attention_masks_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test, attention_masks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluación, generación de comentarios y presentación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo, comparar las predicciones con los valores reales y generar comentarios.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Generar comentarios\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= y_test[i]:\n",
    "        print(f\"Para el ejemplo {i+1}, el contenido es bueno.\")\n",
    "    else:\n",
    "        print(f\"Para el ejemplo {i+1}, el contenido se puede mejorar.\")\n",
    "\n",
    "# Muestra los comentarios junto con las predicciones para proporcionar retroalimentación completa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
